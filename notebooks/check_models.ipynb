{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-04T11:06:50.726102Z",
     "start_time": "2024-12-04T11:06:50.711534Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from RubikCube.src.env import *\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.init import xavier_uniform_\n",
    "\n",
    "class ValuePolicyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ValuePolicyNetwork, self).__init__()\n",
    "\n",
    "        # Define shared layers\n",
    "        self.shared_layers = nn.Sequential(\n",
    "            nn.Linear(480, 4096),\n",
    "            nn.LayerNorm(4096),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(4096, 2048),\n",
    "            nn.LayerNorm(2048),\n",
    "            nn.ELU()\n",
    "        )\n",
    "\n",
    "        # Define value head\n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Linear(2048, 512),  # 2048 -> 512\n",
    "            nn.LayerNorm(512),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(512, 1)  # 512 -> 1 (scalar value)\n",
    "        )\n",
    "\n",
    "        # Define policy head\n",
    "        self.policy_head = nn.Sequential(\n",
    "            nn.Linear(2048, 512),  # 2048 -> 512\n",
    "            nn.LayerNorm(512),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(512, 12)  # 512 -> 12 (policy logits)\n",
    "        )\n",
    "\n",
    "        # Apply Glorot initialization\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            xavier_uniform_(m.weight)  # Glorot initialization for weights\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Transform input\n",
    "        b = x.shape[0]\n",
    "        x = x.view(b, -1)\n",
    "        shared_out = self.shared_layers(x)\n",
    "        value_out = self.value_head(shared_out)\n",
    "        policy_out = self.policy_head(shared_out)\n",
    "        return value_out, policy_out\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amirc\\AppData\\Local\\Temp\\ipykernel_14840\\411434509.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model1.load_state_dict(torch.load('model1.pth'))\n",
      "C:\\Users\\amirc\\AppData\\Local\\Temp\\ipykernel_14840\\411434509.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model2.load_state_dict(torch.load('model2.pth'))\n",
      "C:\\Users\\amirc\\AppData\\Local\\Temp\\ipykernel_14840\\411434509.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model3.load_state_dict(torch.load('model3.pth'))\n",
      "C:\\Users\\amirc\\AppData\\Local\\Temp\\ipykernel_14840\\411434509.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model4.load_state_dict(torch.load('model4.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": "ValuePolicyNetwork(\n  (shared_layers): Sequential(\n    (0): Linear(in_features=480, out_features=4096, bias=True)\n    (1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n    (2): ELU(alpha=1.0)\n    (3): Linear(in_features=4096, out_features=2048, bias=True)\n    (4): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n    (5): ELU(alpha=1.0)\n  )\n  (value_head): Sequential(\n    (0): Linear(in_features=2048, out_features=512, bias=True)\n    (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n    (2): ELU(alpha=1.0)\n    (3): Linear(in_features=512, out_features=1, bias=True)\n  )\n  (policy_head): Sequential(\n    (0): Linear(in_features=2048, out_features=512, bias=True)\n    (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n    (2): ELU(alpha=1.0)\n    (3): Linear(in_features=512, out_features=12, bias=True)\n  )\n)"
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Assuming ValuePolicyNetwork is defined\n",
    "model1 = ValuePolicyNetwork()\n",
    "model2 = ValuePolicyNetwork()\n",
    "model3 = ValuePolicyNetwork()\n",
    "model4 = ValuePolicyNetwork()\n",
    "model_combined = ValuePolicyNetwork()\n",
    "\n",
    "# Load the state dictionaries\n",
    "model1.load_state_dict(torch.load('model1.pth'))\n",
    "model2.load_state_dict(torch.load('model2.pth'))\n",
    "model3.load_state_dict(torch.load('model3.pth'))\n",
    "model4.load_state_dict(torch.load('model4.pth'))\n",
    "\n",
    "# Combine the weights by averaging\n",
    "state_dict1 = model1.state_dict()\n",
    "state_dict2 = model2.state_dict()\n",
    "state_dict3 = model3.state_dict()\n",
    "\n",
    "combined_state_dict = {}\n",
    "for key in state_dict1.keys():\n",
    "    combined_state_dict[key] = (\n",
    "                                       state_dict1[key] + state_dict2[key] + state_dict3[key]\n",
    "                               ) / 3\n",
    "\n",
    "# Load the averaged weights into the combined model\n",
    "model_combined.load_state_dict(combined_state_dict)\n",
    "\n",
    "# Set the model to evaluation mode if needed\n",
    "model1.eval()\n",
    "model2.eval()\n",
    "model3.eval()\n",
    "model4.eval()\n",
    "model_combined.eval()\n",
    "\n",
    "model1.to(device)\n",
    "model2.to(device)\n",
    "model3.to(device)\n",
    "model4.to(device)\n",
    "model_combined.to(device)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-04T11:06:51.747897Z",
     "start_time": "2024-12-04T11:06:50.727549Z"
    }
   },
   "id": "d80c48efbe482b5f",
   "execution_count": 143
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class MetaModel(nn.Module):\n",
    "    def __init__(self, models):\n",
    "        super(MetaModel, self).__init__()\n",
    "        self.models = models\n",
    "        self.shared_layers = nn.Sequential(\n",
    "            nn.Linear(len(self.models) * 13, 2048),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(2048, 4096),\n",
    "            nn.LayerNorm(4096),\n",
    "            nn.ELU()\n",
    "        )\n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Linear(4096, 512),  # 2048 -> 512\n",
    "            nn.LayerNorm(512),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(512, 1)  # 512 -> 1 (scalar value)\n",
    "        )\n",
    "\n",
    "        # Define policy head\n",
    "        self.policy_head = nn.Sequential(\n",
    "            nn.Linear(4096, 512),  # 2048 -> 512\n",
    "            nn.LayerNorm(512),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(512, 12)  # 512 -> 12 (policy logits)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # Transform input\n",
    "        b = x.shape[0]\n",
    "        x = x.view(b, -1)\n",
    "        \n",
    "        vs, ps = [], []\n",
    "        for model in self.models:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                v, p = model(x)\n",
    "            vs.append(v)\n",
    "            ps.append(p)\n",
    "        vs_tensor = torch.stack(vs)\n",
    "        ps_tensor = torch.stack(ps)\n",
    "        x = torch.cat((vs_tensor, ps_tensor), dim=-1).view(-1).to(device)\n",
    "        shared_out = self.shared_layers(x)\n",
    "        value_out = self.value_head(shared_out)\n",
    "        policy_out = self.policy_head(shared_out)\n",
    "        return value_out, policy_out"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-04T11:06:51.763945Z",
     "start_time": "2024-12-04T11:06:51.750445Z"
    }
   },
   "id": "a11edfa1b5d19f19",
   "execution_count": 144
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MeanVectorModel(nn.Module):\n",
    "    def __init__(self, models):\n",
    "        super(MeanVectorModel, self).__init__()\n",
    "        self.models = models\n",
    "\n",
    "    def forward(self, x):\n",
    "        vs, ps = [], []\n",
    "        for model in self.models:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                v, p = model(x)\n",
    "                vs.append(v)\n",
    "                ps.append(p.softmax(dim=-1))\n",
    "\n",
    "        # Compute the mean vectors for v and p\n",
    "        v = torch.stack(vs, dim=0).mean(dim=0)\n",
    "        p = torch.stack(ps, dim=0).mean(dim=0)\n",
    "\n",
    "        return v, p"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-04T11:16:51.289950Z",
     "start_time": "2024-12-04T11:16:51.275574Z"
    }
   },
   "id": "cf083f25414d487d",
   "execution_count": 156
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import random\n",
    "\n",
    "# Assuming Cube class and move definitions are already provided\n",
    "\n",
    "def generate_samples(k: int, l: int):\n",
    "    samples = []\n",
    "    for _ in range(l):\n",
    "        cube = Cube()\n",
    "        actions = []\n",
    "        # Make random moves up to depth k\n",
    "        for _ in range(k):\n",
    "            appended = False\n",
    "            move_index = 0\n",
    "            while not appended:\n",
    "                move_index = random.randint(0, 11)\n",
    "                if len(actions) == 0:\n",
    "                    appended = True\n",
    "                    continue\n",
    "                if move_index%2 == 0 and actions[-1] == move_index + 1:\n",
    "                    continue\n",
    "                if move_index%2 == 1 and actions[-1] == move_index - 1:\n",
    "                    continue\n",
    "                appended = True\n",
    "            cube.move(move_index)\n",
    "            actions.append(move_index)\n",
    "            state = (deepcopy(cube.get_state()), deepcopy(actions))\n",
    "            samples.append(state)\n",
    "\n",
    "    # Transform samples to dictionary format\n",
    "    samples_dict = []\n",
    "    for state, actions in samples:\n",
    "        sample_dict = {\n",
    "            \"state\": [state[0], state[1]],\n",
    "            \"actions\": actions\n",
    "        }\n",
    "        samples_dict.append(sample_dict)\n",
    "\n",
    "    return samples_dict\n",
    "\n",
    "\n",
    "def reward(cube: Cube, action):\n",
    "    new_cube = deepcopy(cube)\n",
    "    new_cube.move(action)\n",
    "    if new_cube.is_solved():\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "\n",
    "def custom_loss(y_vi_pred, y_pi_pred, y_vi, y_pi, weight, alpha=1):\n",
    "    # Compute per-sample losses for value and policy\n",
    "    loss_v = nn.MSELoss(reduction='none')(y_vi_pred, y_vi)  # Shape: (batch_size,)\n",
    "    loss_p = nn.CrossEntropyLoss(reduction='none')(y_pi_pred, y_pi)  # Shape: (batch_size,)\n",
    "\n",
    "    # Apply weights to the per-sample losses\n",
    "    weighted_loss_v = (loss_v * weight).mean()\n",
    "    weighted_loss_p = (loss_p * weight).mean()\n",
    "\n",
    "    # Combine losses\n",
    "    return weighted_loss_v + alpha * weighted_loss_p"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-04T11:06:51.795400Z",
     "start_time": "2024-12-04T11:06:51.782091Z"
    }
   },
   "id": "38d34640b6e5815e",
   "execution_count": 146
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(samples, model, batch_size, epochs, optimizer, loss_fn, device):\n",
    "    for epoch in range(epochs):\n",
    "        bar = tqdm(range(0, len(samples), batch_size), desc=\"Training\")\n",
    "        running_loss = 0\n",
    "        for step, i in enumerate(bar):\n",
    "            batch = samples[i:i + batch_size]\n",
    "\n",
    "            values = torch.zeros(len(batch)).to(device)\n",
    "            policies = torch.zeros((len(batch), 12)).to(device)\n",
    "            data = torch.zeros((len(batch), 20, 24)).to(device)\n",
    "            weights = torch.zeros(len(batch)).to(device)\n",
    "\n",
    "            for j, sample in enumerate(batch):\n",
    "                children_vi = torch.zeros(12).to(device)\n",
    "                children_pi = []\n",
    "\n",
    "                cube = Cube(sample['state'][0], sample['state'][1])\n",
    "\n",
    "                for action in range(12):\n",
    "                    child_cube = deepcopy(cube)\n",
    "                    r = reward(child_cube, action)\n",
    "\n",
    "                    child_cube.move(action)\n",
    "                    corners, edges = child_cube.get_state()\n",
    "                    cube_representation = torch.concat([corners, edges], dim=0)\n",
    "                    cube_representation_encoded = F.one_hot(cube_representation, num_classes=24).float().to(device)\n",
    "                    with torch.no_grad():\n",
    "                        v, p = model(cube_representation_encoded.unsqueeze(0))\n",
    "\n",
    "                    children_vi[action] = v + r\n",
    "                    children_pi.append(p)\n",
    "\n",
    "                target_vi = torch.max(children_vi)\n",
    "                target_pi = F.one_hot(torch.argmax(children_vi), num_classes=12).float()\n",
    "\n",
    "                values[j] = target_vi\n",
    "                policies[j] = target_pi\n",
    "                corners, edges = cube.get_state()\n",
    "                cube_representation = torch.concat([corners, edges], dim=0)\n",
    "                cube_representation_encoded = F.one_hot(cube_representation, num_classes=24).float().to(device)\n",
    "                data[j] = cube_representation_encoded\n",
    "\n",
    "                weights[j] = 1 / len(sample['actions'])\n",
    "\n",
    "            vi_pred, pi_pred = model(data)\n",
    "            pi_pred = pi_pred.view(-1, 12)\n",
    "            loss = loss_fn(vi_pred, pi_pred, values, policies, weights)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            bar.set_description(f'loss: {running_loss / (step + 1):4f}, {loss.item()}')\n",
    "            optimizer.step()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-04T11:06:51.825570Z",
     "start_time": "2024-12-04T11:06:51.798643Z"
    }
   },
   "id": "dfe73ef5cca89d05",
   "execution_count": 147
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "ensemble_models = [model1, model2, model3]\n",
    "meta_model = MetaModel(ensemble_models).to(device)\n",
    "mean_vector_model = MeanVectorModel(ensemble_models).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-04T11:16:56.553554Z",
     "start_time": "2024-12-04T11:16:56.454110Z"
    }
   },
   "id": "418d5c8f8b684555",
   "execution_count": 157
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.666881, 0.21742305159568787: 100%|██████████| 200/200 [00:15<00:00, 12.96it/s]\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import RMSprop, AdamW\n",
    "\n",
    "optimizer = AdamW(meta_model.parameters(), lr=2e-4, weight_decay=2e-4)\n",
    "# optimizer = RMSprop(model.parameters(), lr=2e-4)\n",
    "loss_fn = custom_loss\n",
    "\n",
    "\n",
    "samples = generate_samples(20, 10)\n",
    "# Shake the samples\n",
    "random.shuffle(samples)\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "models = [model1, model2, model3, model_combined]\n",
    "\n",
    "train(samples, meta_model,batch_size, 1, optimizer, loss_fn, device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-04T11:07:07.407282Z",
     "start_time": "2024-12-04T11:06:51.905412Z"
    }
   },
   "id": "a77ddc07569a732e",
   "execution_count": 149
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "def benchmark(models: list, device, iterations=1000, max_scrambles=20):\n",
    "    solved_times = np.array([0] * (len(models)))\n",
    "    for model in models:\n",
    "        model.to(device)\n",
    "\n",
    "    # Add tqdm progress bar for iterations\n",
    "    for iter in tqdm(range(iterations), desc=\"Benchmarking Progress\"):\n",
    "        steps = [random.randint(0, 11) for _ in range(random.randint(1, max_scrambles))]\n",
    "        # All models\n",
    "        for m_idx, model in enumerate(models):\n",
    "            cube = Cube()\n",
    "            for step in steps:\n",
    "                cube.move(step)\n",
    "\n",
    "            i = 0\n",
    "            solution = []\n",
    "            while not cube.is_solved():\n",
    "                model.eval()\n",
    "                corners, edges = cube.get_state()\n",
    "                cube_representation = torch.concat([corners, edges], dim=0)\n",
    "                cube_representation_encoded = F.one_hot(cube_representation, num_classes=24).float().to(device)\n",
    "                with torch.no_grad():\n",
    "                    v, p = model(cube_representation_encoded.unsqueeze(0))\n",
    "                action = torch.argmax(p).item()\n",
    "                solution.append(action)\n",
    "                cube.move(action)\n",
    "                i += 1\n",
    "\n",
    "                if i >= 30:\n",
    "                    break\n",
    "\n",
    "            if cube.is_solved():\n",
    "                solved_times[m_idx] += 1\n",
    "\n",
    "    return solved_times / iterations"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-04T11:09:25.741851Z",
     "start_time": "2024-12-04T11:09:25.722921Z"
    }
   },
   "id": "c5294000dc8e8c10",
   "execution_count": 154
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Benchmarking Progress: 100%|██████████| 1000/1000 [02:54<00:00,  5.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.339 0.343 0.344 0.115 0.353]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "models = [model1, model2, model3, model4, mean_vector_model]#, meta_model]\n",
    "accuracies = benchmark(models, device)\n",
    "print(accuracies)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-04T11:20:03.420409Z",
     "start_time": "2024-12-04T11:17:08.587365Z"
    }
   },
   "id": "230f8427da6b8a38",
   "execution_count": 158
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-04T11:07:35.645011Z",
     "start_time": "2024-12-04T11:07:35.630443Z"
    }
   },
   "id": "88eb26837dcb6e07",
   "execution_count": 151
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-04T11:07:35.660946Z",
     "start_time": "2024-12-04T11:07:35.646089Z"
    }
   },
   "id": "8eb2e00668a9e8b1",
   "execution_count": 151
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
